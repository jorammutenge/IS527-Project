{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad57ce9",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "- Determine the topics in the conversations of chosen characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65016a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "427c3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = ['episode1', 'episode2', 'episode3', 'episode4', 'episode5', 'episode6']\n",
    "data = pd.concat([pd.read_excel('data.xlsx', sheet_name = sheet) for sheet in sheets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa95a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (data\n",
    "     .assign(dialogue=lambda df_: df_.dialogue.str.replace('\\s+', ' ', regex=True).str.strip())\n",
    "     .assign(sorted_pairs=lambda df_: df_.apply(lambda df_: '-'.join(sorted([df_['initiator'], df_['responder']])), axis=1))\n",
    "     .assign(weight=lambda df_: df_.groupby(['sorted_pairs'])['sorted_pairs'].transform('count'))\n",
    "     .assign(dialogue=lambda df_: df_.groupby(['sorted_pairs'])['dialogue'].transform(lambda df_: ' '.join(df_.unique())))\n",
    "     .drop_duplicates('sorted_pairs')\n",
    "     .drop(columns='sorted_pairs')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ed6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# func to add topics\n",
    "def add_topic_labels(df):\n",
    "    # create a list of documents from the 'script' column\n",
    "    documents = list(df['dialogue'])\n",
    "\n",
    "    # define stop words, tags to remove, and words to remove\n",
    "    stop_words = stopwords.words('english')\n",
    "    removal = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "    remove_words = ['like','gone','know','right','na','gon','yeah','really','okay','even','sure','miller','lisa',\n",
    "                    'get','gonna','well','thank','oh','uh','hi','got','um','go','little','mortimer','mayfield','though',\n",
    "                    'would','great','come','hey','wanna','hmm','mr','yes','good','give','jorgy','coleman','ruth','also',\n",
    "                    'going','want','let','think','one','us','look','make','ed','gotta','ben','dale','randolph','actually',\n",
    "                    'something','back','see','need','man','say','sorry','could','madoff','wrong','eric','robert','saw',\n",
    "                    'thing','lt','lf','made','way','said','sir','two','new','naomi','ha','ok','around','jean','ron','mm','onto',\n",
    "                    'louis','winthorpe','gekko','gordon','buddy','jordan','tell','erin','sam','eric','seen','michael','seth',\n",
    "                    'patrick','paul','allen','listen','god','bateman','marcus','heh','bernie','big','steve','jimmy','mrs',\n",
    "                    'anything','christie','patrick','dorsia','garfield','mark','maybe','nick','donnie','name','went'\n",
    "                   ]\n",
    "\n",
    "    # remove stop words and unwanted words\n",
    "    tokenized_docs = [[token for token in gensim.utils.simple_preprocess(doc) if token not in stop_words and token not in remove_words] for doc in documents]\n",
    "\n",
    "    # remove tags\n",
    "    tagged_docs = [nltk.pos_tag(doc) for doc in tokenized_docs]\n",
    "    tokenized_docs = [[token for token, pos in doc if pos not in removal] for doc in tagged_docs]\n",
    "\n",
    "    # create a dictionary from the tokenized documents\n",
    "    dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "    # create a bag-of-words representation of the documents\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "    # train an LDA model on the corpus\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=5,\n",
    "                                                random_state=42,\n",
    "                                                passes=10)\n",
    "\n",
    "    # extract the top fifteen words for each topic\n",
    "    topic_labels = []\n",
    "    for doc in tokenized_docs:\n",
    "        bow = dictionary.doc2bow(doc)\n",
    "        topics = lda_model.get_document_topics(bow)\n",
    "        top_fifteen_words = [dictionary[word] for word, prob in sorted(lda_model.get_topic_terms(max(topics, key=lambda x: x[1])[0], topn=15), key=lambda x: x[1], reverse=True)]\n",
    "        topic_labels.append(top_fifteen_words)\n",
    "\n",
    "    # add the topic labels as a new column in the dataframe\n",
    "    df['topic'] = topic_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d57ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names to create DataFrames for\n",
    "chosen_names = ['NICOLE', 'TANYA', 'RACHEL', 'SHANE', 'ARMOND', 'PAULA', 'OLIVIA']\n",
    "\n",
    "# Loop through the chosen names and create a new DataFrame for each one\n",
    "for name in chosen_names:\n",
    "    # Select rows where the name appears in either initiator or responder\n",
    "    new_df = df[(df['initiator'] == name) | (df['responder'] == name)]\n",
    "    \n",
    "    # Create a new DataFrame with the selected rows\n",
    "    globals()[f'{name}_df'] = pd.DataFrame(new_df)\n",
    "    \n",
    "# func to add word_count column\n",
    "def tweak_df(df):\n",
    "    return (df\n",
    "             .assign(word_count=lambda df_: df_.dialogue.str.len().astype('int16'))\n",
    "             .astype({'weight': 'int8'})\n",
    "             .sort_values('word_count', ascending=False)\n",
    "             .reset_index()\n",
    "             .drop(columns='index')\n",
    "            )\n",
    "\n",
    "# list of dataframes to tweak\n",
    "tweak_these = [NICOLE_df, TANYA_df, RACHEL_df, SHANE_df, ARMOND_df, PAULA_df, OLIVIA_df]\n",
    "\n",
    "for i in range(len(tweak_these)):\n",
    "    tweak_these[i] = tweak_df(tweak_these[i])\n",
    "\n",
    "NICOLE_df, TANYA_df, RACHEL_df, SHANE_df, ARMOND_df, PAULA_df, OLIVIA_df = tweak_these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a88fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NICOLE\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'NICOLE_OLIVIA':0, 'NICOLE_MARK':1, 'RACHEL_NICOLE':2}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    NICOLE_df_key = pd.DataFrame([[NICOLE_df.loc[value, 'initiator'], NICOLE_df.loc[value, 'responder'],\n",
    "                                   NICOLE_df.loc[value, 'dialogue']]],\n",
    "                                 columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = NICOLE_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eca4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TANYA\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'TANYA_BELINDA':0, 'GREG_TANYA':1, 'TANYA_SHANE':2}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    TANYA_df_key = pd.DataFrame([[TANYA_df.loc[value, 'initiator'], TANYA_df.loc[value, 'responder'],\n",
    "                                  TANYA_df.loc[value, 'dialogue']]],\n",
    "                                columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = TANYA_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RACHEL\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'RACHEL_SHANE':0, 'KITTY_RACHEL':1, 'BELINDA_RACHEL':6}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    RACHEL_df_key = pd.DataFrame([[RACHEL_df.loc[value, 'initiator'], RACHEL_df.loc[value, 'responder'],\n",
    "                                   RACHEL_df.loc[value, 'dialogue']]],\n",
    "                                 columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = RACHEL_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fcdbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHANE\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'ARMOND_SHANE':1, 'SHANE_KITTY':2, 'SHANE_OLIVIA':5}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    SHANE_df_key = pd.DataFrame([[SHANE_df.loc[value, 'initiator'], SHANE_df.loc[value, 'responder'],\n",
    "                                  SHANE_df.loc[value, 'dialogue']]], columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = SHANE_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a109873",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ARMOND\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'ARMOND_MARK':1, 'ARMOND_DILLON':2, 'ARMOND_LANI':3}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    ARMOND_df_key = pd.DataFrame([[ARMOND_df.loc[value, 'initiator'], ARMOND_df.loc[value, 'responder'], ARMOND_df.loc[value, 'dialogue']]], columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = ARMOND_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e61c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PAULA\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'OLIVIA_PAULA':0, 'PAULA_KAI':1, 'MARK_PAULA':4}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    PAULA_df_key = pd.DataFrame([[PAULA_df.loc[value, 'initiator'], PAULA_df.loc[value, 'responder'], PAULA_df.loc[value, 'dialogue']]], columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = PAULA_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLIVIA\n",
    "# create mapping to create dataframe for each movie\n",
    "characters = {'MARK_OLIVIA':2, 'TANYA_OLIVIA':3, 'OLIVIA_QUINN':6}\n",
    "\n",
    "# loop through dictionary to create dataframes\n",
    "for key, value in characters.items():\n",
    "    OLIVIA_df_key = pd.DataFrame([[OLIVIA_df.loc[value, 'initiator'], OLIVIA_df.loc[value, 'responder'], OLIVIA_df.loc[value, 'dialogue']]], columns=['initiator', 'responder', 'dialogue']).pipe(add_topic_labels)\n",
    "    globals()[key] = OLIVIA_df_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47244fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes\n",
    "dataframes = [MARK_OLIVIA, TANYA_OLIVIA, OLIVIA_QUINN, OLIVIA_PAULA, PAULA_KAI, MARK_PAULA,\n",
    "              ARMOND_MARK, ARMOND_DILLON, ARMOND_LANI, ARMOND_SHANE, SHANE_KITTY, SHANE_OLIVIA,\n",
    "              RACHEL_SHANE, KITTY_RACHEL, BELINDA_RACHEL, TANYA_BELINDA, GREG_TANYA, TANYA_SHANE,\n",
    "              NICOLE_OLIVIA, NICOLE_MARK, RACHEL_NICOLE\n",
    "             ]\n",
    "\n",
    "\n",
    "# Initialize an empty dataframe to store the concatenated data\n",
    "conv_df = pd.DataFrame()\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "for df in dataframes:\n",
    "    conv_df = pd.concat([conv_df, df], axis=0)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "conv_df = conv_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_df.loc[0, ['topic']].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1b8d9",
   "metadata": {},
   "source": [
    "## Topic conversations of interesting characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ef837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rcParams\n",
    "\n",
    "\n",
    "# SET VIZ TEMPLATE\n",
    "# colors\n",
    "YELLOW='yellow'\n",
    "BLACK='#1a1a1a'\n",
    "GREY='#696969'\n",
    "WHITE='white'\n",
    "\n",
    "# fonts\n",
    "futura_font_path = 'graphfonts/Futura/futura-medium-condensed.ttf'\n",
    "paypal_font_path = 'graphfonts/Paypal/PayPalSansSmall-Regular.ttf'\n",
    "FUTURA = fm.FontProperties(fname=futura_font_path)\n",
    "PAYPAL = fm.FontProperties(fname=paypal_font_path)\n",
    "\n",
    "DPI=300\n",
    "\n",
    "#func to plot topics\n",
    "def plot_word_grid(words, suptitle):\n",
    "    # Define the figure size and background color\n",
    "    fig = plt.figure(figsize=(8, 4), facecolor=BLACK, dpi=DPI)\n",
    "\n",
    "    # Set the title\n",
    "    fig.suptitle(suptitle, fontproperties=FUTURA, color='white', fontsize=12, y=.93)\n",
    "\n",
    "    # Define the layout of the subplots\n",
    "    layout = (5, 5)\n",
    "\n",
    "    # Create subplots and add words to them\n",
    "    for i, word in enumerate(words):\n",
    "        ax = fig.add_subplot(*layout, i+1)\n",
    "        ax.set_facecolor(BLACK)\n",
    "        ax.text(0.5, 0.5, word, fontsize=10, ha='center', va='center', fontproperties=PAYPAL, color=YELLOW)\n",
    "\n",
    "        # Remove spines and tick labels\n",
    "        for side in 'top,right,bottom,left'.split(','):\n",
    "            ax.spines[side].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    \n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06638605",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[9, 'topic'], 'ARMOND and SHANE conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[10, 'topic'], 'SHANE and KITTY conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[19, 'topic'], 'NICOLE and MARK conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[20, 'topic'], 'RACHEL and NICOLE conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f44f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[4, 'topic'], 'PAULA and KAI conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[15, 'topic'], 'TANYA and BELINDA conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b2757",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[13, 'topic'], 'KITTY and RACHEL conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[16, 'topic'], 'GREG and TANYA conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[3, 'topic'], 'OLIVIA and PAULA conversation topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_grid(conv_df.loc[2, 'topic'], 'OLIVIA and QUINN conversation topics')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
